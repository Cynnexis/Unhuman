{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 *-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unhuman Project #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started - Eager ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, I will write the python code given by the [TensorFlow Tutorial: Getting Started - Eager](https://www.tensorflow.org/get_started/eager)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unhuman'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4a472f709e4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munhuman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstarted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIris\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munhuman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStopwatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStopwatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unhuman'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from unhuman.tutorials.getstarted.Iris import Iris\n",
    "from unhuman.utils.Stopwatch import Stopwatch\n",
    "\n",
    "sw_eager = Stopwatch(start_now=True)\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable eager execution (because I'm sooo impatient!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n",
    "sw_eager.stop()\n",
    "\n",
    "print(\"TensorFlow version: \" + tf.VERSION)\n",
    "print(\"Eager execution: {0} (took {1:.2f}s)\".format(str(tf.executing_eagerly()), sw_eager.elapsed()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorFlow version: 1.8.0\n",
    "Eager execution: True (took 14.01s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sw_eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Classification Problem ###\n",
    "\n",
    "Download the dataset: The first 4 numbers are the sepal length, sepal width, petal length and petal width. The last one is the label (see enum Iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "train_dataset_local_file_path = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
    "                                                        origin=train_dataset_url)\n",
    "\n",
    "print(\"A copy of the iris training dataset has been downloaded at from {0} to {1}.\".format(train_dataset_url,\n",
    "                                                                                    train_dataset_local_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Iris enum for more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function parse a CSV line of the training data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noinspection PyShadowingNames\n",
    "def parse_csv_line(line: str) -> tuple:\n",
    "\t\"\"\"\n",
    "\tParse one line of a CSV file using tensorflow.\n",
    "\t:param line: The line of the CSV. It must contains five floats, separated by commas.\n",
    "\t:return: Return the features and the label of the line\n",
    "\t\"\"\"\n",
    "\ttemplate = [[0.], [0.], [0.], [0.], [0]]\n",
    "\tparsed_line = tf.decode_csv(line, template)\n",
    "\t\n",
    "\tfeatures = tf.reshape(parsed_line[:-1], shape=(4,))\n",
    "\tlabel = tf.reshape(parsed_line[-1], shape=())\n",
    "\t\n",
    "\treturn features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will be used as an argument for the training data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file (line by line)\n",
    "training_dataset = tf.data.TextLineDataset(train_dataset_local_file_path)\n",
    "\n",
    "# Skip the header\n",
    "training_dataset = training_dataset.skip(1)\n",
    "\n",
    "# Parse each row using the function 'parse_csv_line'\n",
    "training_dataset = training_dataset.map(parse_csv_line)\n",
    "\n",
    "# Shuffle the entries (work best)\n",
    "training_dataset = training_dataset.shuffle(buffer_size=1000)\n",
    "\n",
    "# The number of examples in the batch is 32\n",
    "training_dataset = training_dataset.batch(batch_size=32)\n",
    "\n",
    "# Print an example from a batch\n",
    "features, label = iter(training_dataset).__next__()\n",
    "print(\"Example from a batch:\\n\\tfeatures = {}\\n\\tlabel = {}\".format(str(features[0]), str(label[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Example from a batch:\n",
    "\tfeatures = tf.Tensor([5.1 3.8 1.9 0.4], shape=(4,), dtype=float32)\n",
    "\tlabel = tf.Tensor(0.0, shape=(), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a model (as described earlier, 4 numbers (sepal length & width, petal length & width, and a label)\n",
    "![Model used in this example](https://www.tensorflow.org/images/custom_estimators/full_network.png)\n",
    "The model is a fully-connected Neural Network because it is very useful to find the relationship between the features\n",
    "(sepal and petal) and the label.\n",
    "In this example, keras will be used as a model selector.\n",
    "Two fully-connected hidden layers containing 10 neurons each will be created. The output contains 3 neurons.\n",
    "The activation function is ReLU : f(x) = {0 if x <= 0 ; x if x > 0}.\n",
    "\\begin{equation*}\n",
    "f(x) = \\begin{Bmatrix}\n",
    "0 & if x \\leq 0\\\\ \n",
    "x & if x > 0\n",
    "\\end{Bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "\ttf.keras.layers.Dense(units=10, activation=\"relu\", input_shape=(4,)), # First hidden layers, with 4 for inputs\n",
    "\ttf.keras.layers.Dense(units=10, activation=\"relu\"), # Second hidden layers, input automatically calculated\n",
    "\ttf.keras.layers.Dense(units=3)  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ###\n",
    "\n",
    "Note: if there is too much entries in the training set, then the model won't generalizable (see overfitting)\n",
    "\n",
    "Now, create a function to compute the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noinspection PyShadowingNames\n",
    "def error(model: tf.keras.models.Model, x, y):\n",
    "\t\"\"\"\n",
    "\tCompute the error between the value returned by model(x) and the expected result y\n",
    "\t:param model: The model to use\n",
    "\t:param x: The input(s)\n",
    "\t:param y: The expected value that the model is supposed to return with x as input(s)\n",
    "\t:return: The error between the actual value and the expected value\n",
    "\t\"\"\"\n",
    "\tcomputed_y = model(x)\n",
    "\treturn tf.losses.sparse_softmax_cross_entropy(labels=y, logits=computed_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a method that, using the `error()` method, record the error for the backpropagation, and allow the model to learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noinspection PyShadowingNames\n",
    "def grad(model: tf.keras.models.Model, inputs, targets):\n",
    "\t\"\"\"\n",
    "\tRecord operations for backpropagation using the error() function.\n",
    "\t:param model:\n",
    "\t:param inputs:\n",
    "\t:param targets:\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\terror_value = error(model, inputs, targets)\n",
    "\treturn tape.gradient(error_value, model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the neural network needs an optimizer to minimize the error (using grad())\n",
    "![Difference between few optimizer algorithms](https://tensorflow.org/images/opt1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)  # This hyperparameter can be adjust for better result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it! The neural network is ready to be fed by the dataset and learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record results at each iteration\n",
    "\n",
    "track_error = []\n",
    "track_accuracy = []\n",
    "\n",
    "max_iterations = 201\n",
    "\n",
    "sw_training_loop = Stopwatch(start_now=True)\n",
    "print(\"\")\n",
    "for iteration in range(max_iterations):\n",
    "\titeration_error_average = tfe.metrics.Mean()\n",
    "\titeration_accuracy = tfe.metrics.Accuracy()\n",
    "\t\n",
    "\t# Training loop\n",
    "\tfor x, y in training_dataset:\n",
    "\t\t# Optimize the model\n",
    "\t\tgrads = grad(model, x, y)\n",
    "\t\toptimizer.apply_gradients(zip(grads, model.variables),\n",
    "\t\t                          # See https://docs.python.org/3.3/library/functions.html#zip\n",
    "\t\t                          global_step=tf.train.get_or_create_global_step())\n",
    "\t\t\n",
    "\t\t# Track progress\n",
    "\t\titeration_error_average(error(model, x, y))\n",
    "\t\titeration_accuracy(tf.argmax(input=model(x), axis=1, output_type=tf.int32), y)\n",
    "\t\n",
    "\t# Stop iteration\n",
    "\ttrack_error.append(iteration_error_average.result())\n",
    "\ttrack_accuracy.append(iteration_accuracy.result())\n",
    "\t\n",
    "\tif iteration % 50 == 0:\n",
    "\t\tprint(\"Iteration n°{0:03d}:\\n\\tError: {1:.3f}\\n\\tAccuracy: {2:.3%}\\n\".format(iteration, track_error[-1],\n",
    "\t\t                                                                             track_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Iteration n°000:\n",
    "    Error: 1.161\n",
    "    Accuracy: 36.667%\n",
    "\n",
    "Iteration n°050:\n",
    "    Error: 0.325\n",
    "    Accuracy: 93.333%\n",
    "\n",
    "Iteration n°100:\n",
    "    Error: 0.209\n",
    "    Accuracy: 96.667%\n",
    "\n",
    "Iteration n°150:\n",
    "    Error: 0.148\n",
    "    Accuracy: 97.500%\n",
    "\n",
    "Iteration n°200:\n",
    "    Error: 0.119\n",
    "    Accuracy: 97.500%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_training_loop.stop()\n",
    "\n",
    "print(\"Training time: {:0.2f}s\".format(sw_training_loop.elapsed()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Graph ###\n",
    "Plot the errors and accuracy in diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noinspection PyTypeChecker\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle(\"Training Metrics\")\n",
    "\n",
    "axes[0].set_ylabel(\"Error\", fontsize=14)\n",
    "axes[0].plot(track_error)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Iteration\", fontsize=14)\n",
    "axes[1].plot(track_accuracy)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following result:\n",
    "![Accuracy and Error accoridng to the Iteration]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_url = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "test_dataset_local_file_path = tf.keras.utils.get_file(fname=os.path.basename(test_dataset_url),\n",
    "                                                       origin=test_dataset_url)\n",
    "\n",
    "print(\"A copy of the iris test dataset has been downloaded at from {0} to {1}.\".format(test_dataset_url,\n",
    "                                                                                       test_dataset_local_file_path))\n",
    "\n",
    "test_dataset = tf.data.TextLineDataset(test_dataset_local_file_path)\n",
    "test_dataset = test_dataset.skip(1)\n",
    "test_dataset = test_dataset.map(parse_csv_line)\n",
    "test_dataset = test_dataset.shuffle(1000)\n",
    "test_dataset = test_dataset.batch(32)\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "test_accuracy = tfe.metrics.Accuracy()\n",
    "\n",
    "for x, y in test_dataset:\n",
    "\tprediction = tf.argmax(model(x), axis=1, output_type=tf.int32)\n",
    "\ttest_accuracy(prediction, y)\n",
    "\n",
    "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))\n",
    "assert test_accuracy.result() >= 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Test set accuracy: 100.000%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTIONS ###\n",
    "\n",
    "Now that our model is trained and tested, we can use it to make prediction on new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = tf.convert_to_tensor([\n",
    "\t[5.1, 3.3, 1.7, 0.5],\n",
    "\t[5.9, 3.0, 4.2, 1.5],\n",
    "\t[6.9, 3.1, 5.4, 2.1]\n",
    "])\n",
    "\n",
    "# Pass the predict_dataset (without label) into the model (the neural network in this case)\n",
    "predictions = model(predict_dataset)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "for i, logits in enumerate(predictions):\n",
    "\tclass_idx = tf.argmax(logits).numpy()\n",
    "\tiris = Iris.from_int(class_idx)\n",
    "\tprint(\"\\tExample n°{}: Prediction: {} (logits = {})\".format(i, Iris.to_str(iris), logits))\n",
    "\t\n",
    "\tif i == 0:\n",
    "\t\tassert iris == Iris.SETOSA\n",
    "\telif i == 1:\n",
    "\t\tassert iris == Iris.VERSICOLOR\n",
    "\telif i == 2:\n",
    "\t\tassert iris == Iris.VIRGINICA\n",
    "\telse:\n",
    "\t\traise AssertionError(\"Iteration n°{} not expected\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Predictions:\n",
    "\tExample n°0: Prediction: Iris setosa (logits = [  3.5572357    0.05438219 -10.779386  ])\n",
    "\tExample n°1: Prediction: Iris versicolor (logits = [-3.243308    1.7982299  -0.28065988])\n",
    "\tExample n°2: Prediction: Iris virginica (logits = [-5.9193463  2.2777824  4.0160313])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
