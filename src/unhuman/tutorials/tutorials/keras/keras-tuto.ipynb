{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unhuman Project #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial - Keras ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, I will write the python code given by the [TensorFlow Tutorial: Keras Guide](https://www.tensorflow.org/guide/keras)\n",
    "\n",
    "Keras is an API to help build and train artificial intelligence model, implemented in TensorFlow library. The *model* is a group of layers which contains neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Started with Keras ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, to import keras, the following code must be added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use keras through ```tf.keras``` module.\n",
    "\n",
    "Note that you might need to install the ```keras``` python package by using:\n",
    "```bash\n",
    "pip install keras\n",
    "```\n",
    "or\n",
    "```bash\n",
    "conda install -n myenv keras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n",
      "Keras version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: {0}\\nKeras version: {1}\".format(tf.__version__, keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Simple Model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Model ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code build a Sequential model, with a fully-connected neural network (multi-layer perceptron):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# Add a dense layer containing 64 neurons\n",
    "model.add(keras.layers.Dense(units=64, activation=\"relu\", input_shape=(32,)))\n",
    "\n",
    "# Add another\n",
    "model.add(keras.layers.Dense(units=64, activation=\"relu\"))\n",
    "\n",
    "# Add a softmax layer with 10 output neurons\n",
    "model.add(keras.layers.Dense(units=10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, The ```relu``` activation function, also called **Rectifer function** returns 0 if the inputs ```x``` is less or equal to zero, or ```x``` otherwise. In other words, the function will conserve the effect during the training process. The following formula shows the function in a more mathematical way:\n",
    "\\begin{equation*}\n",
    "f(x) = \\begin{Bmatrix}\n",
    "0 & if x \\leq 0\\\\ \n",
    "x & if x > 0\n",
    "\\end{Bmatrix} = max(0, x)\n",
    "\\end{equation*}\n",
    "![Retifier function](../../../../../res/images/relu.png)\n",
    "\n",
    "---\n",
    "\n",
    "The **Softmax function**, on the other hand, help to train classification model. The function $\\sigma(z)_{j}$ takes as input a vector. The parameter $j$ is the index of the output neuron ($j = 1, 2, ..., K$). The following equation represents the Softmax function:\n",
    "\\begin{equation*}\n",
    "\\sigma(z)_{j} = \\frac{e^{z_{j}}}{\\sum_{k=1}^{K} e^{z_k}}\n",
    "\\end{equation*}\n",
    "\n",
    "---\n",
    "\n",
    "Another known activation function which is not in this python code is **Sigmoid function**. This function is widely use in artificial intelligence, and it is one of the most activation functions.\n",
    "\\begin{equation*}\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation*}\n",
    "![Sigmoid function](../../../../../res/images/sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amongst all [```tf.keras.layers```](https://www.tensorflow.org/api_docs/python/tf/keras/layers) constructors, there are common arguments that must be detailed before delve into it:\n",
    "* **```activation=```:** The activation function of the neurons for this specific layer. It can be a function name (```str```) or the built-in function it-self (ex: ```tf.keras.layers.Dense(units=10, activation=tf.sigmoid)```).\n",
    "* **```kernel_initialize=```:** The layer's weights at initialization, for every neuron in the layer (ex: ```kernel_initializer=\"orthogonal\"```).\n",
    "* **```biad_initialize=```:** The layer's bias weight only at initialization, for every neuron in the layer (ex: ```bias_initializer=tf.keras/initializers.constant(2.0)```).\n",
    "* **```kernel_regularizer=```:** The regularization that modify the layer's weights (ex: ```kernel_regularizer=tf.keras.regularizers.l1(0.01)```). By default, there is no regularization.\n",
    "* **```bias_regularizer=```:** The regularization that modify the layer's bias weight (ex: ```bias_regularizer=tf.keras.regularizers.l2(0.01)```). By default, there is no regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method [```tf.keras.Model.compile```](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) allow the developer to configure the training process. The most relevant arguments are:\n",
    "* **```optimzer=```:** The optimizer that will train the model. See the module [```tf.train```](https://www.tensorflow.org/api_docs/python/tf/train) for more detail, and have a look at [```AdamOptimizer```](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer), [```RMSPropOptimizer```](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer) and [```GradientDescentOptimizer```](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer) which are the three most known optimizer in TensorFlow.\n",
    "* **```loss=```:** The function that compute the error between the computed value by the model, and the expected value (label). The code below is the error function in the [TensorFlow eager tutorial](https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough#define_the_loss_and_gradient_function) that can be passed as argument for this parameter:\n",
    "```python\n",
    "def error(model, x, y):\n",
    "\t\"\"\"\n",
    "\tCompute the error between the value returned by model(x) and the expected result y\n",
    "\t:param model: The model to use\n",
    "\t:param x: The input(s)\n",
    "\t:param y: The expected value that the model is supposed to return with x as input(s)\n",
    "\t:return: The error between the actual value and the expected value\n",
    "\t\"\"\"\n",
    "\tcomputed_y = model(x)\n",
    "\treturn tf.losses.sparse_softmax_cross_entropy(labels=y, logits=computed_y)\n",
    "```\n",
    "* **```metrics=```:** The metric to monitor and evaluate during the training/testing process. For more detail about this argument, please [see the documentation about it](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile). This parameter accept string, list of string, dictionary (containing string as value) or function type (which are listed [here](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)). A typical example is ```metrics=[\"accuracy\"]``` to monitor the accracy level, or ```metrics=[\"mae\"]``` to watch the *mean absolute error*.\n",
    "\n",
    "For more examples about this method, please have a look at the [TensorFlow tutorial page](https://www.tensorflow.org/guide/keras#set_up_training).\n",
    "\n",
    "Now, let's configure the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.001),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Fit* Training ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, a small dataset will be used to train the model (as it doesn't have any purpose). Fortunatly, TensorFlow has implemented a function for that instead of passing through a training-loop. And this alternative is [```model.fit()```](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) methods, which take 5 important parameters:\n",
    "* **```x=```:** The inputs (*features*) (an array of array of int, for instance)\n",
    "* **```y=```:** The expected outputs (*labels*)\n",
    "* **```batch_size=```:** The number of *slices* of inputs to cut for each iteration. A typical value is ```32```. Warning: The last batch might be very small if the number of samples is not divisible by ```batch_size```.\n",
    "* **```epochs=```:** Number of iterations through the whole dataset input.\n",
    "* **```verbose=```:** (*Optional*) Set the log level: 0 for quiet mode, 1 to display a progressbar (default), or 2 for one line per epoch.\n",
    "* **```validation_data=```:** (*Optional*) Tuple of features and labels. The ```fit``` function will compute the error made by the model at each iteration using this dataset and print it.\n",
    "\n",
    "Time for training the model. I will use the library [NumPy](https://www.numpy.org/) to generate a random dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 347us/step - loss: 11.6075 - acc: 0.1150 - val_loss: 11.6078 - val_acc: 0.1060\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 11.5483 - acc: 0.1110 - val_loss: 11.6036 - val_acc: 0.1050\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 98us/step - loss: 11.5406 - acc: 0.1310 - val_loss: 11.6025 - val_acc: 0.0960\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 98us/step - loss: 11.5343 - acc: 0.1270 - val_loss: 11.6026 - val_acc: 0.1020\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 103us/step - loss: 11.5302 - acc: 0.1360 - val_loss: 11.6021 - val_acc: 0.1080\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 112us/step - loss: 11.5243 - acc: 0.1450 - val_loss: 11.6034 - val_acc: 0.0960\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 108us/step - loss: 11.5203 - acc: 0.1550 - val_loss: 11.6023 - val_acc: 0.1070\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 100us/step - loss: 11.5161 - acc: 0.1760 - val_loss: 11.6030 - val_acc: 0.1140\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 102us/step - loss: 11.5110 - acc: 0.1650 - val_loss: 11.6071 - val_acc: 0.1110\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 97us/step - loss: 11.5065 - acc: 0.1650 - val_loss: 11.6073 - val_acc: 0.1020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f469af2c940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = np.random.random((1000, 32)) # Generate an array containing 1000 array which contain 32 values each\n",
    "labels   = np.random.random((1000, 10)) # array containing 1000 arrays containing 10 values\n",
    "\n",
    "val_features = np.random.random((1000, 32))\n",
    "val_labels   = np.random.random((1000, 10))\n",
    "\n",
    "model.fit(\n",
    "    x=features,\n",
    "    y=labels,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(val_features, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you execute this code, you must see above the output given by the ```fit``` function after computing the validation data at each iteration. You may found a very high loss value (~11.5) and an accuracy value very low (~0.1). The answer of this paradox is that the training dataset and the validation dataset are both **randomly generated**, and then they might not have any link between those two sets. So don't worry if you find such values.\n",
    "\n",
    "Now let's see how to train the model with larger dataset. In this case, the ```fit``` function must take the class [```tf.data.Dataset```](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). For more information about the Dataset class, please see the [tutorial related to that subject](https://www.tensorflow.org/guide/datasets). An additional argument is required for this case: **```steps_per_epoch```** take an integer that will set the number of training step to run before moving to the next iteration. Finally, as ```Dataset``` is a TensorFlow-type, there is no need to specify the batch as it is wrapped in the ```Dataset``` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while training model\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-7-d90b06a579a9>\", line 15, in <module>\n",
      "    validation_steps=3\n",
      "  File \"/home/valentin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\", line 955, in fit\n",
      "    batch_size=batch_size)\n",
      "  File \"/home/valentin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\", line 754, in _standardize_user_data\n",
      "    exception_prefix='input')\n",
      "  File \"/home/valentin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training_utils.py\", line 90, in standardize_input_data\n",
      "    data = [standardize_single_array(x) for x in data]\n",
      "  File \"/home/valentin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training_utils.py\", line 90, in <listcomp>\n",
      "    data = [standardize_single_array(x) for x in data]\n",
      "  File \"/home/valentin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training_utils.py\", line 25, in standardize_single_array\n",
      "    elif x.ndim == 1:\n",
      "AttributeError: 'RepeatDataset' object has no attribute 'ndim'\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "dataset = dataset.batch(32).repeat()\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_features, val_labels))\n",
    "val_dataset = val_dataset.batch(32).repeat()\n",
    "\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    model.fit(\n",
    "        x=dataset,\n",
    "        epochs=10,\n",
    "        steps_per_epoch=30,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=3\n",
    "    )\n",
    "except (ValueError, AttributeError) as e:\n",
    "    logging.exception(\"Error while training model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation & Prediction ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it! The model is finally ready to evaluate and predict some stuff! For that, the methods [```model.evaluate()```](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate) and [```model.predict()```](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict) will be used.\n",
    "\n",
    "The evaluation test the model by taking *features* and *labels* as argument, and print the accuracy of the artificial intelligence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 29us/step\n",
      "[11.499515228271484, 0.173]\n",
      "1000/1000 [==============================] - 0s 34us/step\n",
      "[11.530293426513673, 0.095]\n"
     ]
    }
   ],
   "source": [
    "# Use same dataset as for training\n",
    "print(model.evaluate(features, labels, batch_size=32))\n",
    "\n",
    "try:\n",
    "    model.evaluate(dataset, steps=30)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Use new dataset\n",
    "eval_features = np.random.random((1000, 32))\n",
    "eval_labels   = np.random.random((1000, 10))\n",
    "\n",
    "print(model.evaluate(eval_features, eval_labels, batch_size=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the loss and accuracy values show that the model is not well-trained (because of the randomly-generated datasets)\n",
    "\n",
    "Now, let's see the model making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11069123 0.10190696 0.09946288 ... 0.08772218 0.09612855 0.10351599]\n",
      " [0.09694731 0.12679659 0.09947062 ... 0.08486484 0.09469884 0.09670925]\n",
      " [0.08241725 0.1080385  0.09340771 ... 0.11289827 0.098519   0.0972159 ]\n",
      " ...\n",
      " [0.10156006 0.10767496 0.1047217  ... 0.09856814 0.09940234 0.10035412]\n",
      " [0.10589682 0.10590004 0.09201646 ... 0.09842134 0.0940019  0.09853192]\n",
      " [0.09517039 0.11262779 0.11501919 ... 0.07984426 0.09563505 0.11859612]]\n",
      "[[0.10792921 0.11201751 0.10585713 0.09933899 0.09584751 0.09979638\n",
      "  0.10214879 0.08704121 0.09579197 0.09423132]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(features, batch_size=32))\n",
    "print(model.predict(np.random.random((1, 32)), batch_size=32))\n",
    "\n",
    "try:\n",
    "    model.predict(dataset, steps=30)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [```tf.keras.Sequential```](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) class to generate a model is cute, but not very effective all the time. Depending on the problem, other type of model might be selected (for more information about Keras model, see the [functional API guide](https://keras.io/getting-started/functional-api-guide/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save & Load ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we created a model, train it, evaluate it, to finally make prediction. But imagine now that the training is an incredibly long process (this is the case in the real world), same for the evaluation phase. Do you believe that each time your computer or your server will train it from scratch at every startup? Of course, it would be a waste of time. Fortunatly for us, TensorFlow developers implemented a way to save and restore our model. In this example, my model will be saved in this folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../../../../res/saved_models/keras_tuto_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorFlow team created the method [```model.save_weights()```](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save_weights) to save the model weights, so you only have to run the training process once. Then, you can call [```model.load_weights()```](https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights) to load your model every time you need to! Ideally, a small evaluation would be perfect to assess the loaded model. The code below described this very simple in only 3 lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "model.save_weights(model_path + \"_weights\")\n",
    "model.load_weights(model_path + \"_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have see that the python package h5py is required. If you are on Debian/Ubuntu, you must run the following command:\n",
    "```bash\n",
    "sudo apt-get install libhdf5-dev\n",
    "```\n",
    "Then, the python package h5py must be installed in your environment. If you use pip, you can run this command:\n",
    "```bash\n",
    "pip install h5py\n",
    "```\n",
    "If you use an Anaconda environment, run that command:\n",
    "```bash\n",
    "conda install -n myenv h5py\n",
    "```\n",
    "Please the the [Stack Overflow forum about this subject for more details](https://stackoverflow.com/questions/43385565/importerrorsave-weights-requires-h5py).\n",
    "\n",
    "The file created is a [TensorFlow checkpoint file](https://www.tensorflow.org/guide/checkpoints), but we can specify to save our model according to the Keras HDF5 file format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error while saving|loading model\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-92659b39e074>\", line 2, in <module>\n",
      "    model.save_weights(model_path + \"_weights.h5\", save_format=\"h5\")\n",
      "TypeError: save_weights() got an unexpected keyword argument 'save_format'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.save_weights(model_path + \"_weights.h5\", save_format=\"h5\")\n",
    "    model.load_weights(model_path + \"_weights.h5\")\n",
    "except TypeError:\n",
    "    logging.exception(\"Error while saving|loading model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we saved only the **weights** of the model, but now we will save the configuration only as a JSON or YAML format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"config\": [{\"config\": {\"activity_regularizer\": null, \"units\": 64, \"kernel_regularizer\": null, \"name\": \"dense_1\", \"use_bias\": true, \"trainable\": true, \"activation\": \"relu\", \"dtype\": \"float32\", \"bias_constraint\": null, \"kernel_initializer\": {\"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"seed\": null, \"distribution\": \"uniform\"}, \"class_name\": \"VarianceScaling\"}, \"bias_regularizer\": null, \"bias_initializer\": {\"config\": {}, \"class_name\": \"Zeros\"}, \"kernel_constraint\": null, \"batch_input_shape\": [null, 32]}, \"class_name\": \"Dense\"}, {\"config\": {\"activity_regularizer\": null, \"units\": 64, \"kernel_regularizer\": null, \"name\": \"dense_2\", \"use_bias\": true, \"trainable\": true, \"activation\": \"relu\", \"bias_constraint\": null, \"kernel_initializer\": {\"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"seed\": null, \"distribution\": \"uniform\"}, \"class_name\": \"VarianceScaling\"}, \"bias_regularizer\": null, \"bias_initializer\": {\"config\": {}, \"class_name\": \"Zeros\"}, \"kernel_constraint\": null}, \"class_name\": \"Dense\"}, {\"config\": {\"activity_regularizer\": null, \"units\": 10, \"kernel_regularizer\": null, \"name\": \"dense_3\", \"use_bias\": true, \"trainable\": true, \"activation\": \"softmax\", \"bias_constraint\": null, \"kernel_initializer\": {\"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"seed\": null, \"distribution\": \"uniform\"}, \"class_name\": \"VarianceScaling\"}, \"bias_regularizer\": null, \"bias_initializer\": {\"config\": {}, \"class_name\": \"Zeros\"}, \"kernel_constraint\": null}, \"class_name\": \"Dense\"}], \"keras_version\": \"2.2.0\", \"class_name\": \"Sequential\", \"backend\": \"tensorflow\"}\n"
     ]
    }
   ],
   "source": [
    "# Get the JSON string format of the model:\n",
    "json_format = model.to_json()\n",
    "print(json_format)\n",
    "\n",
    "# Write it\n",
    "with open(model_path + \"_config.json\", 'w') as f:\n",
    "    f.write(json_format + \"\\n\")\n",
    "\n",
    "del json_format\n",
    "\n",
    "# Read it\n",
    "json_format = None\n",
    "with open(model_path + \"_config.json\", 'r') as f:\n",
    "    json_format = f.read()\n",
    "\n",
    "assert json_format != None\n",
    "\n",
    "new_model = keras.models.model_from_json(json_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's serialize the model in YAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backend: tensorflow\n",
      "class_name: Sequential\n",
      "config:\n",
      "- class_name: Dense\n",
      "  config:\n",
      "    activation: relu\n",
      "    activity_regularizer: null\n",
      "    batch_input_shape: !!python/tuple [null, 32]\n",
      "    bias_constraint: null\n",
      "    bias_initializer:\n",
      "      class_name: Zeros\n",
      "      config: {}\n",
      "    bias_regularizer: null\n",
      "    dtype: float32\n",
      "    kernel_constraint: null\n",
      "    kernel_initializer:\n",
      "      class_name: VarianceScaling\n",
      "      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "    kernel_regularizer: null\n",
      "    name: dense_1\n",
      "    trainable: true\n",
      "    units: 64\n",
      "    use_bias: true\n",
      "- class_name: Dense\n",
      "  config:\n",
      "    activation: relu\n",
      "    activity_regularizer: null\n",
      "    bias_constraint: null\n",
      "    bias_initializer:\n",
      "      class_name: Zeros\n",
      "      config: {}\n",
      "    bias_regularizer: null\n",
      "    kernel_constraint: null\n",
      "    kernel_initializer:\n",
      "      class_name: VarianceScaling\n",
      "      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "    kernel_regularizer: null\n",
      "    name: dense_2\n",
      "    trainable: true\n",
      "    units: 64\n",
      "    use_bias: true\n",
      "- class_name: Dense\n",
      "  config:\n",
      "    activation: softmax\n",
      "    activity_regularizer: null\n",
      "    bias_constraint: null\n",
      "    bias_initializer:\n",
      "      class_name: Zeros\n",
      "      config: {}\n",
      "    bias_regularizer: null\n",
      "    kernel_constraint: null\n",
      "    kernel_initializer:\n",
      "      class_name: VarianceScaling\n",
      "      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "    kernel_regularizer: null\n",
      "    name: dense_3\n",
      "    trainable: true\n",
      "    units: 10\n",
      "    use_bias: true\n",
      "keras_version: 2.2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the YAML string format of the model:\n",
    "yaml_format = model.to_yaml()\n",
    "print(yaml_format)\n",
    "\n",
    "# Write it\n",
    "with open(model_path + \"_config.yaml\", 'w') as f:\n",
    "    f.write(yaml_format + \"\\n\")\n",
    "\n",
    "del yaml_format\n",
    "\n",
    "# Read it\n",
    "yaml_format = None\n",
    "with open(model_path + \"_config.yaml\", 'r') as f:\n",
    "    yaml_format = f.read()\n",
    "\n",
    "assert yaml_format != None\n",
    "\n",
    "new_model = keras.models.model_from_yaml(yaml_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entire model ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most of the case, you will save the **entire model**, including the **weights**, the **configuration** and the **optimizer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 363us/step - loss: 11.5878 - acc: 0.1120\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 11.5491 - acc: 0.1100\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 11.5381 - acc: 0.1430\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 85us/step - loss: 11.5329 - acc: 0.1430\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 11.5294 - acc: 0.1620\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 83us/step - loss: 11.5236 - acc: 0.1760\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 84us/step - loss: 11.5185 - acc: 0.1590\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 11.5173 - acc: 0.1660\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 11.5114 - acc: 0.1840\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 11.5083 - acc: 0.1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valentin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/saving.py:127: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n",
      "/home/valentin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/saving.py:270: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(units=64, activation=\"relu\", input_shape=(32,)))\n",
    "model.add(keras.layers.Dense(units=64, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(units=10, activation=\"softmax\"))\n",
    "model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.001),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(features, labels, batch_size=32, epochs=10)\n",
    "\n",
    "model.save(model_path + \".h5\")\n",
    "model = keras.models.load_model(model_path + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
